# Design a Key value store in Go 

A key-value store is a simple but powerful way to store and 
retrieve data. A distributed key-value store allows multiple 
machines to collaborate to store the data, providing fault-tolerance 
and scalability. In this article we will build a distributed 
key-value store in Go using the Raft Consensus algorithm. 

## Design 

Our distributed key-value store will consist of a cluster of nodes that 
communicate with each other using the Raft consensus algorithm. 
Each node will store a subset of the data and will be responsible 
for replicating the data to the other nodes in the cluster. 

To ensure that the data is consistent accross all nodes, we will use 
The Raft consensul algorithm to elect a leader node that is responsible 
for coordinating the replication of the data. The leader node will receive 
Put and Get requests from clients and will replicate the requests to the 
other nodes in the cluster. 

In the event that the leader node fails, the other nodes will elect 
a new leader using the Raft consensus algorithm, ensuring that the system 
remains fault-tolerant

## Implementation 

We will implement our distributed key-value store using the following Go packages: 

    - github.com/hashicorp/raft: A Go implemnetation of the Raft consensus algorithm 
    - github.com/hashicop/raft-boltdb: A Go implementation of the Raft log store using BoltDB 
    - github.com/golang/protobuf: A Go implmentation of the Protocol Buffers data format. 
  
We will create a Node struct that represents a node in the cluster. The node struct will contain 
an in-memory map to store the key-value pairs, as well as a raft.Raft instance to handle the 
Raft consensus algorithm. 

```go 

type Node struct{
    id  string 
    addr string 
    store map[string]string 
    raft *raft.Raft 
    logger *log.Logger 
}

``` 

We will also create a command strcut that represents a Put or 
Get command that can be replicated to the other nodes in the 
cluster. 

```go 

type Command struct{
    Op string 
    Key string 
    Value string 
}

``` 

To start a node in the cluster, we will create a NewNode function that 
takes an ID, address, and list of other node addresses in the cluster. 
The function will create a raft.Raft instance and register a Command log 
entry type. 

```go 

func NewNode(id string, addr string, peers []string) (*Node, error){
    config := raft.DefaultConfig() 
    config.LocalID = raft.ServerID(id) 
    config.Logger = log.New(os.Stdout, "", log.LstdFlags) 

    store := make(map[string]string) 

    fsm := &fsm{
        store: store, 
    }

    raftLog, err := raftboltdb.NewBoltStore(filepath.Join(os.TempDir(), "raft-log")) 
    if err != nil{
        return nil, err 
    }

    snapshotStore := raft.NewDiscardSnapshotStore() 

    transport, err := raft.NewTCPTransport(addr, nil, 3, 10 * time.Second, os.Stdout) 
    if err != nil{
        return nil, err 
    }

    raftNode, err := raft.NewRaft(config, fsm, raftLog, raftLog, snapshotStore, transport) 
    if err != nil{
        return nil, err 
    }

    raftNode.RegisterCommand(&Command{}) 

    node := &Node{
        id: id, 
        addr : addr, 
        store : store, 
        raft: raftNode, 
        logger: config.Logger, 
    }

    joinFuture := node.JoinCluster(peers) 
    if err != joinFuture.Error(); err != nil{
        return nil, err 
    }
}

``` 

The function creates a raft.DefaultConfig instance with the specified ID and logger. 
It also creates an in-memory map to store the key-value pairs. 

Next, the function creates a fsm instance that will handle applying the Command log 
entries to the in-memory map. The fsm struct is defined as follows: 

```go 

type fsm struct{
    store map[string]string 
}

func (f *fsm) Apply(logEngry *raft.Log) interface{}{
    command := &Command{} 

    if err := command.Unmarshal(logEntry.Data); err != nil{
        panic(fmt.Errorf("failed to unmarshal command : %w", err))
    }

    switch command.Op{
    case "put": 
        f.store[command.Key] = command.Value 
    case "get": 
        f.store[command.Key]
    }
    return nil 
}


func (f *fsm) Snapshot() (raft.FSMSsnapshot, error){
    return &fsmSnapshot{
        store: f.store, 
    }, nil 
}

func (f *fsm) Restore(snapshot io.ReadCloser) error{
    defer snapshot.Close() 
    snapshotData, err := ioutil.ReadAll(snapshot) 
    if err != nil{
        return err 
    }

    fsmSnapshot := &fsmSnapshot{} 
    if err := proto.Unmarshal(snapshotData, fsmSnapshot); err != nil{
        return err 
    }

    f.store = fsmSnapshot.store 
    return nil 
}

type fsmSnapshot struct{
    store map[string]string 
}

func (f *fsmSnapshot) Persist(sink raft.SnapshotSink) error{
    defer sink.Close() 
    fsmSsnapshot := &fsmSnsapShotProto{
        Store: f.Store, 
    }

    snapshotData, err := proto.Marshal(fsmSsnapshot) 
    if err != nil{
        return err 
    }
    if _, err := sink.Write(snapshotData); err != nil{
        return err 
    }
    return nil 
}

func (f *fsmSnapshot) Release() {} 

type fsmSnapshotProto struct{
    Store map[string]string `protobuf:"bytes,1,rep,name=store,proto3" json:"store,omitempty"
    protobuf_key:"bytes,1,opt,name=key,proto3" protobuf_val:"bytes,2,opt,name=value,proto3"`
}

``` 

The Apply method applies a Command log entry to the in-memory map by setting the value 
of the key specified in the Command. The snapshot method creates a snapshot of the in-memory 
map, and the Restore method restores the snapshot by unmarshalling the snapshot data and updating 
the in-memory map. 

Next, the function creates a raftboltdb.BoltStore instance to store the Raft log entries, 
a raft.DiscardSnapshotStore instance to discard the Raft snapshots, and a raft. Transport 
instance to communicate with the other nodes in the cluster.

Finally, the function creates a raft.Raft instance with the specified configuration, log store, 
snapshot store, and transport. it also registers the Command log entry type and creates a Node 
instance with the specified ID, address, in memory map, raft.Raft instance, and logger 

To join the cluster, the JoinCluster method is called on the Node instance. the JoinCluster method 
creates a raft.Configuration instance with the IDs and addresses of the other nodes in the cluster, 
and then adds the configuration to the Raft cluster using the raft.Raft instance.

```go 

func (n *Node) JoinCluster(peers []string) *raft.Future{
    configFuture := n.raft.GetConfiguration() 
    if err != configFuture.Error() ; err != nil{
        n.logger.Fatalf("failed to get raft configuration: %s", err)
    }

    config := configFuture.Configuration() 

    for _, peer := range peers{
        if _, ok := config.Servers[raft.ServerID(peer)]; !ok{
            future := n.raft.AddVoter(raft.ServerID(peer), 
            raft.ServerAddress(peer), 0, 0) 
            if err != future.Error(); err != nil{
                n.logger.Fatalf("failed to add voter to %s: %s", peer, err)
            }
        }
    }

    return n.raft.BootstrapCluster(raft.Configuration{Servers: config.Servers, })
}

``` 

The method first gets the current configuration of the Raft cluster using the raft.Raft.GetConfiguration method. it then iterates over the list of peers and adds them to the cluster 
using the raft.Raft.AddVoter method. Finally it boots the cluster using the updated configuration 
with the raft.Raft.BootstrapCluster method. 

To handle Put and Get requests, we will creat a handleCommand method on the Node struct that takes 
a Command as an argument and applies the command to the in-memory map.

```go 

func (n *Node) handleCommand(command *Command) (string, error){
    data, err := proto.Marshal(command) 
    if err != nil{
        return "", err 
    }

    future := n.raft.Apply(data, 10*time.Second) 
    if err := future.Error(); err != nil{
        return "", err 
    }
    switch result := future.Response().(type){
    case string: 
        return result, nil 
    case nil:
        return "", nil 
    default: 
        return "", fmt.Errorf("unexpected response type: %T", result)
    }
}

``` 

The method marshals the Command to a byte slice using the proto.Marshal method 
and applies the byte slice to