## Introduction

Programming Languages are notations for describing computations to people
and to machines. Before a program can be run it first must be translated
into a form in which it can be executed by a computer.
Programs that do this translation are called compilers.

## 1.1 Language Processors

A compiler is a program that can read a program in one language the source
language and translate it into an equivalent program in another language
the target language;
An important role of the compiler is to report any errors in the source program
that it detects during the translation process.

==> An interpreter is another form of language processor. Instead of producing
a target program as a translation, an interpreter appears directly execute the
the operations specified in the source program on inputs supplied by the user.

==> The machine language produced by the compiler is usually much faster than
an interpreter at mapping inputs to outputs. An interpreter however, can usually
give better error diagnostics than a compiler, because it executes the source program
statement by statement.

### Phases of a compiler

==> Symbol Table
==> Character stream ==> Lexical Analyser --> Token stream ==> Syntax Analyser
--> Syntax tree ==> Semantic Analyzer --> Syntax tree ==> Intermediate code generator
--> Intermediate representation ==> Machine Indep Code Optimizer --> Intermediate representatoin ==> Code Generator --> target machine code ==> Machine Dependent Code
Optimizer --> target machine code

## 1.2.1 Lexical Analysis

The first phase of a compiler is called lexical analysis or scanning. The lexical
analyzer reads the stream of characters making up the source program and groups
the characters into meaningful sequences called lexemes. For each lexeme, the
lexical analyzer produces as output a token of the form
<token-name, attribute-value>

That it passes on to the subsequent phase, syntax analysis. In the token, the
first component token-name is an abstract symbol used during syntax analysis, and
the second component value points to an entry in the symbol table for this token

For example suppose a source program contains the assignment

  position = initial + rate * 60

The characters in this asssignment could be grouped into the following lexemes
and mapped into the following tokens passed on to the syntax analyzer:

1. position is a lexeme that would be mapped into a token <id, 1> where "id" is
an abstract symbol standing for the identifier and 1 points to the symbol table
entry position. The symbol-table entry for an identifier holds information
about the identifier.

2. The assignment symbol = is a lexeme that is mapped into the token <=>
Since this token needs no attribute value, the second component is omitted.
One could have used the name "assign" for the token name but for notational
convenience we will stick with "="

3. **initial** is a lexeme that is mapped into the token <id, 2>, where 2
points to the symbol table entry for "initial"

4. + is a lexeme that is mapped into the token <+>

5. **rate** is a token that is mapped into the token <id, 3> where 3 points
to the symbol table entry for "rate"

6. * is a lexeme that is mapped to the token <*>

7. 60 is a lexeme that is mapped to the token <60>

In this representation, the token names =, + and * are abstract symbols for
the assignment, addition, and multiplication operators respectively.
Blanks separating the lexemes would be discarded by the lexical analyzer.

<id,1><=><id, 2><+><id, 3><*><60>

In the above representation, the token names = +, and * are abstract symbols
for the assignment, addition and multiplication operators respectively.


### 1.2 Syntax Analysis

The second phase of the compiler is syntax analysis or syntax parsing. The parser
uses the first components of the tokens produced by the lexical analyzer to create
a tree-like intermediate representation that depicts the grammatical structure of the token stream. A typical representation is a syntax tree in which each interior
node represents an operation and the children of the node represent the arguments of
the operation

```go

position = initial  + rate * 60

// Symbol Table
// 1. position =>
// 2. initial =>
// 3. rate =>

// ==> Lexical Aanalyzer
// ==> <id, 1>  <=> <id, 2> <+> <id, 3> <*> <60>
// ==> Syntax Analyzer
//
//         =
//       /    \
// <id, 1>    +
//          /   \
//      <id, 2>   *
//              /    \
//           <id,3>   60
//
// ==> Semantic analyzer
//
//          =
//      /     \
//   <id,1>     +
//            /    \
//        <id,2>    *
//                /   \
//             <id,3>  inttofloat
//                        |
//                       60
//
// ==> Intermediate code generator
// t1 = inttofloat(60)
// t2 = id3 * t1
// t3 = id2 + t2
// id1 = t3
//
// ==> Code Optimizer
//
// t1 = id3 * 60.0
// id1 = id1 + t1
//
// ==> Code generator
//
// LDF R2, id3
// MULF R2, R2, #60.0
// LDF R1, id2
// ADDF R1, R1, R2
// STF id1, R1

```


### 1.2.3 Semantic Analysis

The semantic analyzer uses the syntax tree and the information in the symbol table
to check the source program for semantic consistency with the language definition.
It also gathers type information and saves it in either the syntax tree or the symbol
table, for subsequent use during intermediate code generation phase.

### 1.2.4 Intermediate code generation

In the process of translating the source program into target code, a compiler may
construct one or more intermediate representations, which can have a variety of
forms. Syntax trees are a form of intermediate representation; they are commonly
used during syntax and semantic analysis.
