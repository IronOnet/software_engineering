## Syntax Directed Translator

### Introduction

The analysis phase of a compiler breaks up a source program into constituent
pieces and produces an internal representation for it, called intermediate code.
The synthesis phase translates the intermediate code into the target program.

Besides specifying the syntax of a language, a context-free gram can be used
to help guide the translation of programs.  

===> Source Program ==> Lexical Analyzer ==> Tokens ==> <--> Symbol Table ==> Parser
==> Syntax tree

- A lexical analyzer allos a translator to handle multicharacter constructs like
identifiers, which are written as sequence of characters, but are treated as units
called tokens, during syntax analysis; for example, in the expression "count+1",
the identifier count is treated as a unit.

### 2.2 Syntax Definition

Grammars will be used throught the book to organize compiler front-ends.
A grammar naturally describes the hierarchical structure of most programming
language constructs. For example, an if-else statement in Java can have the
form
"if (expression) statement else statement"

That is, an if-else statement is the concatenation of the keyword "if", an opening
parenthesis, an expression, a closing parenthesis, a statement, the keyword else and
another statement. Using the variable expr to denote an expression and the variable
stmt to denote a statement, this structuing rule can be expressed as

==> stmt --> if(expr) stmt else stmt

In which the array may be read as "can have the form". Such a rule is called a
production. In a production lexical statement like the keyword if and the
parentheses are called terminals.

#### 2.2.1 Definition of Grammars

A context-free grammar has four components:
  1. A set of terminal symbols, sometimes referred to as "tokens". The terminals
  are the elementary symbols of the language defined by the grammar.

  2. A set of nonterminals, sometimes called syntactic variables. Each non-terminal
  represents a set of strings of terminals, in a manner we shall describe.

  3. A set of productions, where each production consists of a nonterminal,
  called the head or left side of the production, an arrow, and a sequence of
  terminals and/ or nonterminals, called the body or right side of the production.
  The intuitive intent of a production is to specify one of the written forms of a
  construct; if the head nonterminal represents a construct, then the body represents
  a written form of the construct.

  4. A designation of one of the nonterminals as the start symbol.
  We specify grammars by listing their productions, with the production for the
  start symbol listed first. We assume that digits, signs such as < and <= and
  boldfase strings such as "while" are terminals. An italicized name is a
  nonterminal, and any nonitiliazed name or symbol may be assumed to be a terminal.

## Derivations

A grammar derives strings by begining with the start symbol and repeatedly replacing
a nonterminal by the body of a production for that nonterminal. The terminal strings
that can be derived from the start symbol from the language defined by the grammar.

Example 2.2: The language defined by the grammar of E


==> Parsing is the problem of taking a string of terminals and figuring out how
to derive it from the start symbol of the grammar, and if it cannot be derived from
the start symbol of the gramar, then reporting syntax errors within the sting.
Parsing is one of the most fundamental problems in all of compiling; the main
approaches to parsing are discussed in note #4

### 2.2.3 Parse Trees

A parse tree pictorially shows how the start symbol of a grammar derives a string
in the language. If nonterminal A has a production A --> XYZ, then a parse tree
may have an interior node labeled A with three children labeled X, Y, and Z, from
left to right

                  A
              /   |   \
              |   |   |
              |   |   |
              X   Y   Z

Formally, given a context-free grammar, a parse tree according to the grammar
is a tree with the following properties:

- The root is labeled by the start symbol.
- Each lef is labeled by a termonal or by ϵ
- Each interior node is labeled by a non terminal.
- If A is the nonterminal labeling some interior node and X_1, X_2, ... X_n are
the labels of the children of that node from left to right, then there must be a
production A --> X_1*X_2...X_n. Here X_1,X_2,... X_n each stand for a symbol that
is either a terminal or nonterminal. As a special case if A --> ϵ is a production, then a node labeled A my have a single child labeled ϵ.

### 2.2.4 Ambiguity

We have to be careful in talking about the structure of a string according to
a grammar. A grammar can have more than one parse tree generating a given
string of terminals. Such a grammar is said to be ambiguous. To show that grammar
is ambigous, all we need to do is find a terminal string that is the yield of more than one parse tree. Since a string with more than one parse tree usually has more
than one meaning, we need to design unambigous grammars for compiling applications
or to use ambigous grammars with additional rules to resolve the ambiguities.

### 2.2.5 Associativity of Operators

By convention, 9+5+2 is equivalent to (9+5)+2 and 9-5-2 is equivalent to
(9-5)-2. When an operand like 5 has operators to its left and right, conventions
are needed for deciding which operator applies to that operand. We say that
the operator + associates to the left, because an operand with plus signs on both
sides of it belongs to the operator to its left. In most programming languages the
four arithmetic operators, addition, subtraction, multiplication and division are
left-associative.

### 2.2.6 Precedence operators

The associativity rule for + and * apply to occurences of the same operator,
so they do not resolve this ambiguity. Rules defining the relative precedence
of operators are needed when more than one kind of operator is present.
