# Scale to millions of users 

## Relational Vs Non-Relational DBs 

Non relational databases might be the right choice if: 
	- Your application requires super-low latency 
	- Your data are unstructured, or you do not have any relational data 
	- You only need to serialize and deserialize data (JSON, XML, YAML, etc.) 
	- You need to store massive amount of data 


## Vertical Scaling Vs Horizontal scaling 

=> Vertical scaling is the process of adding more power (CPU, RAM, etc) to your servers. 
=> Horizontal scaling, referred to as "scale-out", allows you to scale by adding more servers into your 
pool of resources. 

=> When traffic is low, vertical scaling is a great option, and the simplicity of vertical scaling is its 
main advantage. Unfortunately, it comes with serious limitations. 

=> Vertical scaling has a hard limit. 
		- It is impossible to add unlimited CPU and memory to a single server. 
		- Vertical scaling does not have failover and redundancy. If one server goes down, the 
		website/app goes down with it completely. 

=> Horizontal scaling s more desirable for large scale applications due to the limitations of vertical scaling. 

=> If many users access the web server simultaneously and it reaches the web server's load limit, users generally 
experience slower response or fail to connect to the server. A load balancer is the best technique to address 
these problems. 

## Load Balancer 

A load balancer evenly distributes incoming traffeic among web servers that are defined in a load-balanced set.
Users connect to the public IP of the load balancer directly. With this setup, web servers are unreachable directly 
by clients anymore. For better security, private IPs are used for communication between servers.

## Database Replication 

A master database generally only support write operations. A slave database gets copies of the data from the master
database and only supports read operations. All the data-modifying commands like insert, delete, or update must be
sent to the master database. Most application require a much higher ratio of reads to writes; thus the number 
of slave databases in a system is usually larger than the number of master databases.

=> Advantages of database replication: 

	- Better performance: In the master-slave model, all writes and updates happen in master nodes; whereas; read operations are distributed accrsso slave nodes. This model improves performance because it allows more queries to be processed in parallel. 

	- Reliability: If one of your database servers is destroyed by a natural disaster; such as typhoon or an earthquake, data is still preserved. You do not need to worry about data loss because data is replicated accross multiple locations.

	- High availability: By replicating data accross different locations; your website remains


## Cache 

A cache is a temporary storage area that stores the result of expensive responses or frequently accessed data in memory so that subsequent requests are served more quickly. 
Every time a new web page loads, one or more database calls are executed to fetch data. The application performance 
is greatly affected by calling the database repeatedly. The cache can mitigate this problem.

### Cache Tier 

The cache tier is a temporary data store layer, much faster than the database. The benefits of having a separate cache
tier include better system performance, ability to reduce database workloads, and the ability to scale the cache tier
independently. 

### Various Caching Strategies 

> Cache Aside
	- The application first checks the cache 
	- If the data is found in cache, we have a cache hit. The data is read and returnecd to the client 
	- If the data is not found in cache, we have a cache miss. The application has to do some extra work. it queries the database to read the data, returns it to the client and stores the data in cache so the subsequent reads for the same data results in a cache hit.

Cache-aside caches are usually general purpose and work best for ead-heavy workloads. Memcached and Redis 
are widelty used. Systems using cache-aside are resilient to cache failures. If the cache cluster goes down 
the system can still operate by going directly to the database.

When cache-aside is use, the most common write strategy is to write data to the database directly. 
When this happens, cache may become incosistent with the database. To deal with this, developers 
generally use time to liv (TTL) and continue serving stale data until TIL expires. 

> Read-Through cache 

This cache sits inline with the database. When there is a cache miss, it loads missing data from the database, 
populates the cache and returns it to the application.

Read-through caches work best for read-heavy workloads when the same data is requested many times. For example
a news story. The disadventage is that when the data is requested the first time, it always results in cache 
miss and incurs the extra penalty of loading data to the cache. 

> Write-Through cache 

In this write strategy, data is first written to the cache and then to the database. The cache sits in line 
with the database and writes always go through the cache to the main database. 
This helps the cache maintain consistency with the main database.

> Write-Around 

Here, data is written direclty to the database and only the data that is read makews it way to the cache.

Write around can combine with read-through and provides a good performance in situations where data is 
written once and read less frequently or never. For example real time logs or chatroom messages.



> Read-Through Cache 

Read-Through case sits in-line with the database. When there is a cache miss, it loads missing data 
from the database, populates the cache and returns it to the application

> Write-Back or Write-Behind 

Here, the application writes data to the cache which stores the data and acknowleges to 
the application immediately. Then later the cache writes the data back to the database. 

This is very similar to write-Through but there's one crucial difference: in write-Through 
the data written to the cache is synchronously updated in the main database. In write-back 
the data written to the cache is asynchronously updated in the main database.

Write back caches improve write performance and are good for write heavy workloads. When 
combined with read-through, it works good for mixed workloads, where the most recently 
updated and acce data is always available in cache.



### Considerations for using Cache 

	- Consider using a cache when data is read frequenly but modified infrequently. 
	Since cached data is stored in volatile memory, a cache server is not ideal for persisting data.

	- Expiration policy. It is a good practicy to implement an expiration policy. Once cached data is 
	expired, it is removed from the cache. When there is no expiration policy the cached data will be 
	stored in memory permenently

	- COnsistency: This involves keeping the data store and the cache in sync. Inconsistency can happen 
	because data-modifying operations on the data store and cache are not in a single transaction.


	- Mitigating Failures: A single cache server represents a potential single point of failure (SPOF), 
	A single point of failure is a part of a system, that if it fails, will stop the entire system from working.

	- Eviction Policy: Once the cache is full, any requests to add item to the cache might cause existing items
	to be removed. This is called cache eviction. Least-recently-used (LRU) is the most popular eviction policy
	there are alswo (LFU) lest frequently used and (FIFO) First in First out

## Content delivery network (CDN) 

A CDN is a network of geographically dispersed servers used to deliver static content. like images, videos, CSS
Javascript files, etc. 

## Stateless web tier 

To scale the web tier horizontaly we need to move the state (e.g user session data) out of the web tier. 
A good practice is to store session data in the persistent storage such as relational database or NoSQL. 
Each web server in the cluster can access state data from databases.

### Stateful Architecture 

A stateful server remembers client data from one request to the net. A stateless keeps no state information.

### Stateless architecture 

In a stateless arechitecture, HTTP requests from users can be sent to any web servers, which fech state data 
from a shared data store. State data is stored in a shared data store and kept out of web servers.

## Data centers 

In normal operation, users are geoDNS-routed, also known as geo-routed, to the closest data center, with 
a split traffic of x% in US-East and (100-x)% in US-West. geoDNS is a DNS service that allows domain names 
to be resolved to IP addresses based on the location of a user.

## Message Queue 

A message queue is a durable compoent, stored in memory, that supports asynchronous communication. It serves as 
a buffer and distributes asynchronous requests. The basic architecture of a message queue is simple. 
Input services, called producers/publishers create messages, and publish them to a message queue. 
Other services or servers, called consumers/subscribers, connect to the queue, and perform actions 
defined by the messages.

## Logging, metrics and automation 

Logging: monitoring error logs is important because it helps to identify errors and problems in the 
system. You can monitor error logs at per server level or use tools to aggregate them to 
a centralized service for easy search and viewing.

Metrics: Collecting different types of metrics help us to gain business insights and understand the health status 
of the system. Some of the following metrics are useful.
	- Host level metrics: CPU, Memory, disk I/O etc 
	- Aggregated level metrics: for example, the performance of the entire database tier. 
	- Key business metrics: daily active users, retention, revenue etc.


## Database Scaling 

There are two broad approaches for database scaling: vertical scaling and horizontal scaling. 

### Vertical Scaling 

Vertical scaling, also known as scaling up, is the scaling by adding ore power (CPU, RAM, Disk, etc) 
to an existing machine. There are some powerful database servers.

Sharding is a great technique to scale the database


## Summary  

How to scale a system to support millions of users: 

	- Keep web tiers stateless 
	- Build redundancy at every tier 
	- Cache data as much as you can 
	- Support multiple data centers 
	- HOst static assets in CDN 
	- Scale your data tier by sharding 
	- Split tiers into individual services 
	- Monitor your system and use automation tools



