# Design A Rate Limiter 

In a network system, a rate limiter is used to control the rate of traffic sent by a client or 
a service. In the HTTP world, a rate limiter limits the number of client requests allowed 
to be sent over a specified period. 

If the API request count exceeds the threshold defined by the rate limiter, all excess calls 
are blocked. E.g 

	- A user can write no more than 2 posts per second 
	- YOu can create a maximum of 10 accounts per day from the same IP address. 
	- You can claim rewards no more than 5 times per week from the same device. 

What are the benefits of a rate limiter: 

	- Prevent resource starvation caused by Denial of Service attack (DoS). Almost all APIs published 
	by large tech companies enforce some form of rate limiting. For example, Twitter limits the numbr 
	of tweets to 300 per 3 hour 

	- Reduce Cost. Limiting excess requests means fewer servers and allocating more resources to high 
	priority APIs. Rate limiting is extremely important for companies that use paid third parties

	- Prevent servers from beign overloaded: To reduce server load, a rate limiter is used to filter 
	out excess requests caused by bots or user's misbehaviour.


## Step 1: Understand The Problem and establish design scope

Rate limiting can be implemented using different algorithms, each with its own pros and cons. The interactions 
between an interviewer and candidate help to clarify the type  of rate limiters we are trying to build. 

=> Is this a client side or server side rate limiter? R/ server side 
=> Does the rate limiter throttle API requests based on IP, the user ID, or other properties 

R/ It should be flexible enogh to suport different sets of throttle rules. 

=> What is the size of the system? R/ It must be able to handle a large number of requests.

=> Will it work in a distributed environment? R/ Yes 

=> Is the rate limiter a separate service or should it be implemented in the application code? 

R/ the design decision is up to you.

=> Do we inform users who are throttled? R/ yes 

### Requireements 

Here is the summary of the requirements for the system: 

	- Accurately limit excessive requests 
	- Low latency. The rate limiter shoud not slow down HTTP response time. 
	- Use a little memory as possible. 
	- Distributed rate limiting. The rate limiter can be shared accross multiple servers or processes.
	- Exeption handling. Show clear exception to users when their requests are throttled. 
	- High fault tolerance. If there are any problems with the rate limiter (for example, a cache server 
	goes offline) it does not affect the entire system.

## Step 2: Propose a high level design 

=> Where to put the rate limiter? 

	Intuitively, you can implemnt a rate limiter at either the client or server-side. 

	=> CLient-side implementation. Generally speaking, client is an unreliable place to enforce rate 
	limiting because client requests can easily be forged by malicious actors.

	=> Besides the client and server-side implementations, there is an alternative way. Instead of putting 
	a rate limiter at the API servers, we create a rate limiter hardware, which throttles requests to 
	your APIs 

	=> Cloud microservies have become widely popular and rate limiting is usually implemented within 
	a component called API gateway. API gateweay is a fully managed service that supports rate limiting, 
	SSL termination, authentication, IP Whitelisting, servicing static content, etc. 

	=> While designing a rate limiter, an important question to ask ourselves is: where should the 
	rate limiter be implemented, on the server-side or in a gateway? There is no absolute answer


	=> Here are few general guidelines: 

		- Evaluate your current technology stack, such as progrmming language, cache service, etc. 
		Make sure your current programming language is efficient to implement rate limiting on the server-side 

		- identify the rate limiting algorithm that fits your business needs. When you implement everything 
		on the server-side, you have full control of the algorithm. However, your choice might be limited if you 
		use a third-party gateway. 

		- If you have already used microservice architecture and included an API gateway in the design to perform 
		authentication, IP whitelisting etc. you may add a rate limiter to the API gateway 

		- Building your own rate limiting service takes time. If you do not have enough engineering resources 
		to implement a rate limiter, a commercial API gateway is a better option 


### Algorithms for rate limiting 

=> Rate limiting can be implemented using different algorithms, and each of tehm has distinct pros and cons. 

		- Token Bucket 
		- Laking bucket 
		- Fixed window counter 
		- Sliding window log 
		- Sliding window counter 


#### Token bucket algorithm 

This algorithm is widely used for rate limiting. It is simple, well understood and commonly usd by internet 
companies. It works as follows 

	- A token bucket is a container that has predefined capacity. Tokens are put in the bucket at preset rates 
	periodically. Once the bucket is full, no more tokens are added. The refiller puts 2 tokens into the bucket 
	every second. Once the bucket is full, extra tokens will overflow.

	- each request consumes one token. When a request arrives, we check if ther are enough tokens in 
	the bucket. 

	- If there are enough tokens, we take one token out for each request, and the request goes through. 

	- If there are not enough tokens, the request is dropped.

THe token bucket algorithm takes two parameters: 

	- Bucket size: the maximum number of tokens allowed in the bucket 
	- Refill rate: number of tokens put into the bucket every second.

How many buckets do we nee? THis varies, and it depends on the rate-limiting rules. Here are a few examples. 

	- It usually necessary to have different buckets for different API endpoints. 
	- If we need to throttle requests based on IP Addresses, each IP address requires a bucket 
	- If the system allows a maximum of 10.000 requests per second, it makes sense to have a global bucket 
	shared by all requests.

#### Leaking bucket algorithm 

THe leaking bucket algorithm is similar to token bucket except that requests are processed at a fixed rate. 
It is usualy implemented with a first-in-first-out queue (FIFO). it works as follows: 

	- When a request arrives, the system checks if the queue is full. If it is not full, the request 
	is added to the queue. 
	- Otherwise, the request is dropped 
	- Requests are pulled from the queue and proessed at regular intervals.

## Step 3: Design deep dive 

The high-level design does not answer the following questions: 

	- How are rate limiting rules created? where are the rules stored? 
	- How to handle requests that are rate limited?

### Rate limiting rules 

Rules are generally written in configuration files and saved on disk. the specify for example how many 
requests per minute a client is allowed to maked

	- RUles are stored on the disk, workers frequently pull rules from the disk and store them in the cache
	- When a client sends a request to the server, the request is sent to the rate limiter middleware first 
	- Rate limiter middleware loads rules from the cache. It fetches counters and last request timestamp from 
	Redis cache. Based on the response, the rate limiter decides: 
			- If the request is not rate limited, it is forwarded to API servers 
			- If the request is rate limited, the rate liiter returns 429 too many requests 
				error to the client. 
			- In the meantime, the request is either dropped or forwarded to the queue.

### Exceeding the rate limit 

In case a request is rate limited, APIs return a HTTP response code 429 (too many requests) to the client. 
Depending on the use cases, we may enqueue the rate-limited requests to be processed later. 
For example, if some orders are rate liited due to system overload, we may keep those orders to be 
processed later. 

### Rate limiter headers 

How does a client know whether it is being throttled? And how does a client know the number of allowed 
remaining requests, before being throttled? the answers lies in HTTP response headers. The rate limiter 
returns the following HTTP headers to clients: 

	X-Ratelimit-Remaining: The remaining number of allowed requests within the window 
	X-Ratelimit: It indicates how many calls the client can make per time window. 
	X-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again

## Rate limiter in a distributed environment 

Building a rate limiter that works in a single server environment is not difficult. However, scaling 
the system to support multiple servers and concurrent threads is a differnt story. 
There are two challenges: 
		- Race conditions 
		- Syncrhonization issues 

Race conditions can happen in highly concurrent environment, LOcks are the most common obvious solution 
for solving race codition. However locks, will slow the system down.





