# Design a Key-Value Store  

A key-value store, also referred to as key-value database, is a non-relational database. 
Each unique identifier is stored as a key with its associated value. This data pairing 
is known as a "key-value" pair. 

In a Key-Value pair, the key must be unique, and the value associated with the key can 
be accessed through the key. Kyes can be plain text or hashed values. 
FOr performance reasons, a short key works better. What do keys look like? 

    - Plain text key: "last_logged_in_at" 
    - Hashed key: 253DDEC4 

The value in a key-value pari can be strings, list, objects, etc. The value is usually 
treated as an opagque object in key-value stores, such as Amazon dynamo, Memcached, Redis etc. 

===> You are asked to design a key-value store that supports the following operations:  
    - put(key, value) // insert "value" associated with "key" 
    - get(key) // get "value" associated with "key" 


## Understand the problem and establish design scope 

THere is no perfect design. Each design achieves a specific balance regarding the tradeoffs 
of the read, write, and memory usage. Another tradeoff that has to be made is one between 
consitency and availability.  our key-value store will comprise the following characteristics: 

    - The size of a key-value pari is small: less than 10 KB 
    - Ability to store big data. 
    - High availability: THe system responds quickly, even during failures 
    - High scalability: The system can be scaled to support large data set 
    - Automatic scaling: THe addition/deletion of servers should be automatic based on traffic.
    - Tunable consistency 
    - Low latency 


### Single server key-value store 

A single server kv store is easy. An intuitive approach is to store key-value pairs in 
a hash table, which keeps everything in memory. Even though memory access is fast, 
fitting everything in memory is impossible due to space constraints. Two optimizations 
can be done to fit more data in a single server 
    - data compression 
    - Store only frequently used data in memory and the rest on disk 

====> Even with these optimizations, a single server can reach its capacity very quicly. 
A distributed key-value store is required to support big data.


### Distributed key-value store 

A distributed key-value store is also called a distributed hash table, which distribute key-value 
pairs accross many servers. When designing a distributed system, it is important to understand 
the CAP theorem. (Consistency, Availability and Partition tolerance). 

==> The CAP theorem states that it is impossible for a distributed system to simultaneously provide 
more than two of these three guarantees: consistency, availability, and partition tolerance.

=> Consistency means all clients see the same data at the same time no matter which node they connect 
to. 

=> Availability: means any client which requests data gets a response even if some of the nodes are 
down. 

=> Partition Tolerance: A partition indicates a communication break between two nodes. 
Partition tolerance means the system continues to operate despite network partitions. 

CAP theorems states that one of the three properties must be sacrificed to support 2 of the 
3 properties

===> # Ideal situation 

In the ideal world, network partition never occurs. Data written to n1 is automatically 
replicated to n2 and n3. Both consistency and availability are achieved.


===> # Real-world distributed systems

In a distributed system, partitions, cannot be avoided, and when a partition occurs, we must 
choose between consistency and availability. 

If we choose consistency over availability (CP System), we must block all write operations to 
n1 and n2 to avoid data inconsistency among these three servers, which makes the system 
unavailable. Bank systems usually have extremely high consistent requirements. 


## SYstem compoents 

==> Core components to build a key-value store: 

    - Data partition 
    - Data replication 
    - Consistency 
    - Inconsistency resolution 
    - Handling Failures 
    - System architecture diagram 
    - Write path 
    - Read path 

### Data Partition 

=> For large applications, it is infeasible to fit the complete dataset in a single server. 
The simples way to accomplish this is to split the data into smaller partitions and store them i
n multiple servers. There are two challenges while partitioning the data: 
    - Distribute data accross multiple servers evenly 
    - Minimize data movement when nodes are added or removed 

==> Consistent hashing is a great techniwqe to solve these problems. Using consistent hashing to partion data has the following advantages 

    - Automatic scaling: servers could be added and removed automatically depending on the load. 
    - Heterogeneity: The number of virtual nodes for a server is proportional to the server capacity


## Data replication 

To achieve high availability and reliability, data must be replicated asynchronously over N 
servers, where N is a configurable parameter. These N servers are chosen using the following 
logic. After a key is mapped to a position on the hash ring, walk clockwise from that position 
and choose the first N servers on the ring to store data copies. 
In figure 6-5 (N = 3), key0 is replicated at s1, s2, and s3.

With virtual nodes, the first N nodes on the ring may be owned by fewer than N physical servers. 
To avoid this issue, we only choose unique servers while performing the clockwise walk logic. 

Nodes in the same data center often fail at the same time due to power outages, 
network issues, natural disasters, etc. For better reliability, replicas are placed 
in distinct data centers, and data centers are connected through high-speed networks.



## Consistency 

Since data is replicated at multiple nodes, it must be synchronized across replicas. 
Quorum consensus can guarantee consistency for both read and write operations. 


N = The number of replicas 

W = A write quorum of size W. For a write operation to be considered as successful, 
write operation must be acknowledged from W replicas. 

R = A read quorum of size R. For a read operation to be considered as successful, 
read operation must wait for responses from at least R replicas.


### Consistency models 

Consistency model is one other important factor to consider when desinging a 
key-value store. A consistency model defines the degree of data consistency 
and a wide spectrum of possible consistency models exist: 

    - Strong consistency: any read operation returns a value corresponding to 
    the result of the most updagted write data item. A client never sees out of date data. 

    - Weak consistency: Subsequent read operations may not see the most updated value. 
    - Eventual consistency: This is a specific form of weak consistency. Given enough time, 
    all updates are propagated, and all replicas are consistent.

Strong consistency is usually achiveved by foricing a replica not to accept new reads/writes 
until every replica has agreed on current write. This approach is not ideal for higly available 
systems because it could block new operations. 

=> Dynamo and Cassandra adopt eventual consistency, which is the model used for our key-value store.

=> From concurrent writes, eventual consistency allows inconsistent values to enter the system and 
force the client to read the values to reconcile.


## Inconsistency resolution: Versioning 

Replication gives high availability but causes inconsistencies among replicas. Versioning and 
vector locks are used to solve inconsistency problems. Versioning means treating each data 
modification as a new immutable version of data.

Vectors clocks are used to solved version conflicts. A vector clock is a [server, version] pair 
associated with a data item. It can be used to check if one version precedes, succeeds, or is 
in confict with others. 

Assume a vector clock is represented by D([S1, D1], [S2, V2], ..., [S, Vn]), where D 
is the data item, v1 is a version counter, and s1 is a server number, etc. If data 
item D is written to server Si, the system must perform one of the following tasks. 

    - Increment vI if [Si, Vi] exists. 
    - Otherwise, create a new entry [Si, 1] 

=> Let's explain the above abstract logic with a concrete example 

    1. A client writes ad ata item D1 to the system, and the write is handled by server Sx 
   which now has the vector clock D1[(Sx, 1)] 

   2. Another client reads the latest D1, updates it to D2, and writes it back. D2 descends 
    from D1 so it overwrites D1. Assume the write is handled by the same server SX, which now 
    has vector clock D2([Sx, 2])

    3. Another client reads, the latest D2, updates it to D3, and writes it back. Assume the 
   write is handled by server Sy, which now has vector clock D3([Sx, 2], [Sy, 1]) 

   4. Another client reads the latest D2, updates it to D4, and writes it back assume the 
   write is handled by server Sz, which now has D4([Sx, 2], [Sz, 1])


   5. When another client reads D3 and D4, it discovers a conflict, which is caused by 
   data item D2 being modified by both Sy and Sz. The conflict is resolved by the client 
   and updated Data is sent to the server


## Handling  Failures 

### Failure Detection 

IN a distributed system, it is insufficient to believe that a server is down 
because another server says so. Usually, it requires at least wto independent 
sources of information to mark a server down.


### Multicasting

=> This is the straightforward solution, but inefficient when many servers are 
in the system.

#### Gossip Protocol 

Gossip protocol is a failure detection methods that works as follows: 

    - Each node maintains a node membership list, which contains member IDs 
    and heartbeat counters 

    - Each node periodically increments its hearbeat counter 
    - Each node periodically sends heartbeats to a set of random nodes, which in turn 
    propagate to another set of nodes. 
    - Once nodes receive heartbeats, membership list is updated to the latest info.

    - If the heartbeat has not increased for more than predefined periods, the member is considered 
    as offline.

## Handling Temporary Failures 

After failures have been detected through the gossip protocol, the system needs to deploy certain 
mechanisms to ensure availability. In the strict quorum approach, read and write operations 
could be blocked. 

=> Sloppy Quorum is a technnique used to improve availability. Instead of enforcing the quorum 
requirement, the system chooses the first W healthy servers for writes and first R healthy servers 
for reads on the hash ring. Offline servers are ignored.


=> Hinted Handoff: If a server unavailable due to network or server failures, another server will 
process requests temporarily. When the down server is up, changes will be pushed back to achieve 
data consistency.

=> Anti-entropy protocol: When a replica is permanently unavailable, this protocl is implemented 
to keep replicas in sync. It involves comparing each piece of data on replicas and updating each 
replica to the newest version. A merkle tree is used for inconsistency detection and minimizing the 
amount of data transfered.


## Handling Data center outage 

Data center outage could happen due to power outage, network outage, natural disaster, etc. 
To build a system capable of handling data center outage, it is important to replicate 
data accross multiple data centers.


## System Architecture 

=> Main features of the architecture are listed as follows: 

    - Clients communicate with the key-value store through simple APIs: get(key) and put(key, value) 
    - A coordinator is a node that act as a proxy between the client and the key-value store. 
    - NOdes are distributed on a ring using consistent hashing 
    - The system is completely decentralized so adding and moving nodes can be automatic 
    - Data is replicated at multiple nodes 
    - There is no single point of failure as every node has the same set of responsibilities.


### Write path 

What happens after a write request is directed to a specific node.

1. The write request is persisted on a commit log file. 
2. Data is saved in the memory cache.
3. When the memory cache is full or reaches a predefined threshold, data is flushed to 
   an SSTable (A sorted-string table, a sorted list of <key, value> pairs.)

### Read Path 

After a read request is directed to a specific node, it first checks if data is in the memory 
cache. If so, the data is returned to the client. If the data is not in memory, it will be 
retrieved from the disk instead. We need to find out which SSTable contains the key. 
Bloom filter is commonly used to solve this problem. 

1. The system first check if data is in memory. If not go to step 2 
2. If data is not in memory, the system checks the bloom filter 
3. The bloom filter is used to figure out wich SSTables might contain the key 
4. SSTables return the resut of the data set 
5. The result of the dataset is returned to the client.



